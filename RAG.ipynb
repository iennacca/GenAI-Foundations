{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de6abe6d-03b0-4335-b388-fefad3e0172d",
   "metadata": {},
   "source": [
    "# Retrieval augmented generation (RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a57245-e4fe-4b22-9d70-b419523b11a2",
   "metadata": {},
   "source": [
    "## Loading Documents\n",
    "A first step in RAG is to load document. You need a loader that supports the document type you are interested in. We use in this example Langchain, because it includes a collection of 60+ libraries for multiple types of documents and formats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03dc9db-b35b-40d7-b94f-013c6cc41d50",
   "metadata": {},
   "source": [
    "A first example with the `PyPDFLoader` library. Pdf support is direct and a single command is enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "926d4fa1-897d-476a-a16a-caea89e1337a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /opt/conda/lib/python3.12/site-packages (0.3.27)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /opt/conda/lib/python3.12/site-packages (from langchain) (0.3.75)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /opt/conda/lib/python3.12/site-packages (from langchain) (0.3.11)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in /opt/conda/lib/python3.12/site-packages (from langchain) (0.4.26)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/conda/lib/python3.12/site-packages (from langchain) (2.8.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.12/site-packages (from langchain) (2.0.34)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.12/site-packages (from langchain) (2.32.5)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.12/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /opt/conda/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (8.2.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /opt/conda/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.15.0)\n",
      "Requirement already satisfied: packaging>=23.2 in /opt/conda/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (24.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.12/site-packages (from langsmith>=0.1.17->langchain) (0.27.0)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /opt/conda/lib/python3.12/site-packages (from langsmith>=0.1.17->langchain) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /opt/conda/lib/python3.12/site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /opt/conda/lib/python3.12/site-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /opt/conda/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.20.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2025.8.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.12/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.0)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/conda/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: langchain-community in /opt/conda/lib/python3.12/site-packages (0.3.29)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=0.3.75 in /opt/conda/lib/python3.12/site-packages (from langchain-community) (0.3.75)\n",
      "Requirement already satisfied: langchain<2.0.0,>=0.3.27 in /opt/conda/lib/python3.12/site-packages (from langchain-community) (0.3.27)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.12/site-packages (from langchain-community) (2.0.34)\n",
      "Requirement already satisfied: requests<3,>=2.32.5 in /opt/conda/lib/python3.12/site-packages (from langchain-community) (2.32.5)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.12/site-packages (from langchain-community) (6.0.1)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.12/site-packages (from langchain-community) (3.12.15)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /opt/conda/lib/python3.12/site-packages (from langchain-community) (8.2.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.6.7 in /opt/conda/lib/python3.12/site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /opt/conda/lib/python3.12/site-packages (from langchain-community) (2.10.1)\n",
      "Requirement already satisfied: langsmith>=0.1.125 in /opt/conda/lib/python3.12/site-packages (from langchain-community) (0.4.26)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /opt/conda/lib/python3.12/site-packages (from langchain-community) (0.4.1)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /opt/conda/lib/python3.12/site-packages (from langchain-community) (1.26.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.6.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.6.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /opt/conda/lib/python3.12/site-packages (from langchain<2.0.0,>=0.3.27->langchain-community) (0.3.11)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/conda/lib/python3.12/site-packages (from langchain<2.0.0,>=0.3.27->langchain-community) (2.8.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.12/site-packages (from langchain-core<2.0.0,>=0.3.75->langchain-community) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /opt/conda/lib/python3.12/site-packages (from langchain-core<2.0.0,>=0.3.75->langchain-community) (4.15.0)\n",
      "Requirement already satisfied: packaging>=23.2 in /opt/conda/lib/python3.12/site-packages (from langchain-core<2.0.0,>=0.3.75->langchain-community) (24.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.12/site-packages (from langsmith>=0.1.125->langchain-community) (0.27.0)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /opt/conda/lib/python3.12/site-packages (from langsmith>=0.1.125->langchain-community) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /opt/conda/lib/python3.12/site-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /opt/conda/lib/python3.12/site-packages (from langsmith>=0.1.125->langchain-community) (0.23.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /opt/conda/lib/python3.12/site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (0.21.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/conda/lib/python3.12/site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.32.5->langchain-community) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.32.5->langchain-community) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.32.5->langchain-community) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.32.5->langchain-community) (2025.8.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.12/site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.1)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (4.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.0.9)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.3.0)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/conda/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<2.0.0,>=0.3.75->langchain-community) (2.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=0.3.27->langchain-community) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /opt/conda/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=0.3.27->langchain-community) (2.20.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.6.7->langchain-community) (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# For this loading Documents part, you may need these packages installed\n",
    "\n",
    "%pip install langchain\n",
    "%pip install -U langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ba47b07-c15a-407e-abaf-06e13b147f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pypdf in /opt/conda/lib/python3.12/site-packages (6.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import warnings # optional, disabling warnings about versions and others\n",
    "warnings.filterwarnings('ignore') # optional, disabling warnings about versions and others\n",
    "\n",
    "%pip install pypdf \n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"docs/War-of-the-Worlds.pdf\")\n",
    "book = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0155ef27-face-4f42-8359-b36b8aaf3eec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How long is the document we loaded?\n",
    "len(book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be420767-f472-40a2-a239-dc61747a697a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "darkness were Ottershaw and Chertsey and a ll their hundreds of people, sleeping in \n",
      "peace.  \n",
      "   He was full of speculation that night a bout the condition of Mars, and scoffed at the \n",
      "vulgar idea of its having in- habitants w ho were signalling us. His idea was that \n",
      "meteorites might be falling in a heavy shower upon the planet, or that a huge volcanic \n",
      "explosion was in progress. He pointed out to me how unlikely it was that organic \n",
      "evolution had taken the same direction in the two adjacent pl\n"
     ]
    }
   ],
   "source": [
    "#Looking at a small extract, one page, and a few hundred characters in that page\n",
    "page = book[5]\n",
    "print(page.page_content[0:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78963c00-21cd-45fc-9b11-d4c8b695e917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'producer': 'PDFill: Free PDF Writer and Tools',\n",
       " 'creator': 'PyPDF',\n",
       " 'creationdate': '2011-08-24T10:49:19-04:00',\n",
       " 'moddate': '2011-08-24T10:49:19-04:00',\n",
       " 'source': 'docs/War-of-the-Worlds.pdf',\n",
       " 'total_pages': 128,\n",
       " 'page': 5,\n",
       " 'page_label': '6'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Which page is it, from which document?\n",
    "page.metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a16ab2-a33a-4dc3-a58d-8434d549e3a5",
   "metadata": {},
   "source": [
    "A second example with a Youtube video. There is a little more work here. The yt_dlp library will need options to know what audio format to download (we won't care much about the video part). Here we use m4a, at 192 kbps. Then the ffmpeg and ffprobe programs will isolate and stream the audio part. We will then use the OpenAI whisper library to covnert the audio into text (speech-to-text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92996683-f7ae-40bf-8a84-c1fbdbf33e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting yt_dlp\n",
      "  Using cached yt_dlp-2025.9.5-py3-none-any.whl.metadata (177 kB)\n",
      "Using cached yt_dlp-2025.9.5-py3-none-any.whl (3.3 MB)\n",
      "Installing collected packages: yt_dlp\n",
      "  Attempting uninstall: yt_dlp\n",
      "    Found existing installation: yt-dlp 2025.9.5\n",
      "    Uninstalling yt-dlp-2025.9.5:\n",
      "      Successfully uninstalled yt-dlp-2025.9.5\n",
      "Successfully installed yt_dlp-2025.9.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pydub in /opt/conda/lib/python3.12/site-packages (0.25.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: ffmpeg in /opt/conda/lib/python3.12/site-packages (1.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: ffprobe in /opt/conda/lib/python3.12/site-packages (0.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.12/site-packages (2.8.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/lib/python3.12/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch) (78.1.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/conda/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.12/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /opt/conda/lib/python3.12/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /opt/conda/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /opt/conda/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /opt/conda/lib/python3.12/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /opt/conda/lib/python3.12/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /opt/conda/lib/python3.12/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /opt/conda/lib/python3.12/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /opt/conda/lib/python3.12/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /opt/conda/lib/python3.12/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /opt/conda/lib/python3.12/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /opt/conda/lib/python3.12/site-packages (from torch) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /opt/conda/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /opt/conda/lib/python3.12/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /opt/conda/lib/python3.12/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /opt/conda/lib/python3.12/site-packages (from torch) (3.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tiktoken in /opt/conda/lib/python3.12/site-packages (0.11.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.12/site-packages (from tiktoken) (2024.9.11)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.12/site-packages (from tiktoken) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2025.8.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting git+https://github.com/openai/whisper.git\n",
      "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-akisouys\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-akisouys\n",
      "  Resolved https://github.com/openai/whisper.git to commit c0d2f624c09dc18e709e37c2ad90c039a4eb72a2\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: openai-whisper\n",
      "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for openai-whisper: filename=openai_whisper-20250625-py3-none-any.whl size=803979 sha256=bb4cd49befdc74d7f4f95abdeca24aeb6a00746404dc7eb30e1e507c76b5cf3e\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-meq5vtjb/wheels/c3/03/25/5e0ba78bc27a3a089f137c9f1d92fdfce16d06996c071a016c\n",
      "Successfully built openai-whisper\n",
      "Installing collected packages: openai-whisper\n",
      "  Attempting uninstall: openai-whisper\n",
      "    Found existing installation: openai-whisper 20250625\n",
      "    Uninstalling openai-whisper-20250625:\n",
      "      Successfully uninstalled openai-whisper-20250625\n",
      "Successfully installed openai-whisper-20250625\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=2vkJ7v0x-Fs\n",
      "[youtube] 2vkJ7v0x-Fs: Downloading webpage\n",
      "[youtube] 2vkJ7v0x-Fs: Downloading tv simply player API JSON\n",
      "[youtube] 2vkJ7v0x-Fs: Downloading tv client config\n",
      "[youtube] 2vkJ7v0x-Fs: Downloading tv player API JSON\n",
      "[info] 2vkJ7v0x-Fs: Downloading 1 format(s): 251\n",
      "[download] Sleeping 4.00 seconds as required by the site...\n",
      "[download] Destination: docs/youtube/Big Data Architectures.webm\n",
      "[download] 100% of   22.03MiB in 00:00:00 at 36.50MiB/s    \n",
      "[ExtractAudio] Destination: docs/youtube/Big Data Architectures.m4a\n",
      "Deleting original file docs/youtube/Big Data Architectures.webm (pass -k to keep)\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --no-deps --force-reinstall yt_dlp\n",
    "%pip install pydub\n",
    "%pip install ffmpeg\n",
    "%pip install ffprobe\n",
    "%pip install torch\n",
    "%pip install tiktoken\n",
    "%pip install --upgrade --no-deps --force-reinstall git+https://github.com/openai/whisper.git\n",
    "\n",
    "import os\n",
    "import whisper\n",
    "from yt_dlp import YoutubeDL\n",
    "\n",
    "# Step 1: Set up the download options\n",
    "url = \"https://www.youtube.com/watch?v=2vkJ7v0x-Fs\"\n",
    "save_dir = \"docs/youtube/\"\n",
    "output_template = os.path.join(save_dir, '%(title)s.%(ext)s')\n",
    "\n",
    "ydl_opts = {\n",
    "    'format': 'bestaudio/best',\n",
    "    'outtmpl': output_template,  # Save the file to the specified directory with a title-based name\n",
    "    'postprocessors': [{\n",
    "        'key': 'FFmpegExtractAudio',\n",
    "        'preferredcodec': 'm4a',  # You can change this to mp3 if you prefer\n",
    "        'preferredquality': '192',\n",
    "    }],\n",
    "    'ffmpeg_location': '/usr/bin/ffmpeg',  # Specify the location of ffmpeg\n",
    "}\n",
    "\n",
    "\n",
    "# Step 2: Download the audio from the YouTube video\n",
    "with YoutubeDL(ydl_opts) as ydl:\n",
    "    ydl.download([url])\n",
    "\n",
    "# Step 3: Find the downloaded file\n",
    "downloaded_file = [f for f in os.listdir(save_dir) if f.endswith('.m4a')][0]  # Assuming m4a, adjust if using mp3\n",
    "downloaded_file_path = os.path.join(save_dir, downloaded_file)\n",
    "\n",
    "# Step 4: Load the Whisper model\n",
    "model = whisper.load_model(\"base\")  # You can choose 'tiny', 'base', 'small', 'medium', or 'large'\n",
    "\n",
    "# Step 5: Transcribe the audio file\n",
    "result = model.transcribe(downloaded_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be2e169a-66bc-4f9e-bacb-05537633d599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript saved to docs/youtube/transcript.txt\n"
     ]
    }
   ],
   "source": [
    "# Adding metadata to the transcript, and saving the transcript to a file so we can use it outside of this program.\n",
    "class Document:\n",
    "    def __init__(self, source, text, metadata=None):\n",
    "        self.source = source\n",
    "        self.page_content = text\n",
    "        self.metadata = metadata or {}\n",
    "\n",
    "# Wrap the transcription result in the Document class with metadata\n",
    "document = Document(\n",
    "    source=downloaded_file_path,\n",
    "    text=result['text'], \n",
    "    metadata={\"source\": \"youtube\", \"file_path\": downloaded_file_path}\n",
    ")\n",
    "#Save the transcript to a text file\n",
    "transcript_file_path = os.path.join(save_dir, 'transcript.txt')\n",
    "with open(transcript_file_path, 'w') as f:\n",
    "    f.write(result['text'])\n",
    "\n",
    "print(f\"Transcript saved to {transcript_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37e4c04b-8eb6-411a-9876-181447c55824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32850"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many characters in this transcript file?\n",
    "len(document.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6311ecd-14e7-43d0-ae65-9302e22a9b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " In lesson four, we will go deeper into architectures for big data, and we will take a closer look at some of the most popular big data management systems. First, we're going to look at how the big data management system framework looks, and explore the commonalities that pretty much all the big data systems have, as well as some of the key differences between no SQL, MPP, and Hadoop. Next, we're going to take a deep dive into the Hadoop data management system. You will see how we both store dat\n"
     ]
    }
   ],
   "source": [
    "# Print the first 500 characters of the transcript\n",
    "print(document.page_content[:500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641d8b7b-a005-4431-9238-4647e4c89510",
   "metadata": {},
   "source": [
    "## Splitting our documents in chunks\n",
    "A second step is to split our documents (a 128-page book and 32K-character trasncript file) into smaller chunks. We use Langchain libraries here again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3afaa217-7968-4e82-b791-c4ca3c099b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the most important library, recursive character splitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5267876d-56c3-4515-950b-3622e770ca93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunks have a character length, and an overlap values. For example (in real life, you are probably closer to 500 to 1000 and 50 to 100 respectively):\n",
    "rsplit = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=20,\n",
    "    chunk_overlap=5,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \" \", \"\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "356c24ea-0d81-4cce-aff6-b1601e5515c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take an example string\n",
    "text1 = 'abcdefghijklmnopqrstuvwxyz1234567890'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc10a7ed-af57-472a-b937-94cf1ed6b3e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abcdefghijklmnopqrst', 'pqrstuvwxyz123456789', '567890']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rsplit.split_text(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1ea8853-b4d0-4395-ab64-e6dd854c4acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hamlet = \"\"\"Truly to speak, and with no addition, \\\n",
    "We go to gain a little patch of ground \\\n",
    "That hath in it no profit but the name. \\\n",
    "To pay five ducats, five, I would not farm it; \\\n",
    "Nor will it yield to Norway or the Pole \\\n",
    "A ranker rate, should it be sold in fee.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f250a326-40cf-4b99-8818-b388a489900e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Truly to speak, and',\n",
       " 'and with no',\n",
       " 'no addition, We go',\n",
       " 'go to gain a little',\n",
       " 'patch of ground',\n",
       " 'That hath in it no',\n",
       " 'no profit but the',\n",
       " 'the name. To pay',\n",
       " 'pay five ducats,',\n",
       " 'five, I would not',\n",
       " 'not farm it; Nor',\n",
       " 'Nor will it yield',\n",
       " 'to Norway or the',\n",
       " 'the Pole A ranker',\n",
       " 'rate, should it be',\n",
       " 'be sold in fee.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rsplit.split_text(Hamlet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "192d29ed-a7c9-4958-b8f3-b1eadc2cb763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's go for a more realistic chunk size\n",
    "rsplit = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=100,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \" \", \"\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6ba9199-d8cb-4514-88df-fd060a97225d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the files, first the pdf\n",
    "rdoc1 = rsplit.split_documents(book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fbdd4440-fd97-47df-8a5c-ada64dbe0a15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "952"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rdoc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58d132c8-a45a-4ce3-96e5-bed41007aa30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the splitted version has more documents (pages) than the original pdf source, \n",
    "len(book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9be1616b-09a4-4d3c-9f75-7793daebf847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Split 1 ---\n",
      "but that was simply that my eye was tired. Forty millions of miles it was from us--more \n",
      "than forty millions of miles of void. Few people realise the im- mensity of vacancy in \n",
      "which the dust of the material universe swims.  \n",
      "   Near it in the field, I re member, were three faint points of  light, three telescopic stars \n",
      "infinitely remote, and all around it was th e unfathomable darkness of empty space. You\n",
      "\n",
      "--- Split 2 ---\n",
      "infinitely remote, and all around it was th e unfathomable darkness of empty space. You \n",
      "know how that blackness looks on a frosty st arlight night. In a tele- scope it seems far \n",
      "profounder. And invisible to me because it wa s so remote and small, flying swiftly and \n",
      "steadily towards me across that incredible di stance, drawing nearer every min- ute by so \n",
      "many thousands of miles, came the Thing they  were sending us, the Thing that was to\n",
      "\n",
      "--- Split 3 ---\n",
      "many thousands of miles, came the Thing they  were sending us, the Thing that was to \n",
      "bring so much struggle and calamity and death to the earth. I never dreamed of it then as I \n",
      "watched; no one on earth dreamed of that unerring missile.  \n",
      "   That night, too, there was another jetting out of gas from the distant planet. I saw it. A \n",
      "reddish flash at the edge, the slightest proj ection of the outline just as the chronometer\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Printing a few splits\n",
    "for i, doc in enumerate(rdoc1[30:33]):  # Adjust the number 3 to print more or fewer splits\n",
    "    print(f\"--- Split {i + 1} ---\")\n",
    "    print(doc.page_content)\n",
    "    print()  # Print an empty line for better readability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f840a45f-5a73-47f5-a8ed-5d049e7c9491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Split 1 ---\n",
      "how we actually execute analytics jobs on that data that's sitting in HDFS. So on the master node we have a new function, a new demon called the job tracker, and on the slave nodes we have a new one called the task tracker. Now let's say we have an application job that needs to communicate and analyze some data set that's sitting on the slave nodes down below. So the application job executes a Java command on the API, communicating with the name node, and then it tries to communicate down to\n",
      "\n",
      "--- Split 2 ---\n",
      "Java command on the API, communicating with the name node, and then it tries to communicate down to the task trackers below. Now one of the big differences between big data architectures and traditional data processing is that we don't try to bring all the data to one place and analyze it. What we do is we send the processing job down to the data and distribute it. You can think of it like having a lot of minions doing the work for you. One analogy might be if you had a very, very big\n",
      "\n",
      "--- Split 3 ---\n",
      "having a lot of minions doing the work for you. One analogy might be if you had a very, very big newspaper. Let's say you had a newspaper that was 10,000 pages long. And you wanted to find just one keyword in that newspaper. Well how would you do it? Well you could start on page one of that newspaper and read all the way through to page 10,000. And as you go you start to add up and count all the words that you're looking for. But that would take a very, very long time. But now what if you had a\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Splitting the trasncript of the audio file\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Step 1: Load the transcript text\n",
    "transcript_file_path = \"docs/youtube/transcript.txt\"\n",
    "with open(transcript_file_path, 'r') as f:\n",
    "    transcript_text = f.read()\n",
    "\n",
    "# Step 2: Create a Document object\n",
    "document = Document(page_content=transcript_text)\n",
    "\n",
    "# Step 3: Split the transcript into chunks\n",
    "rdoc2 = rsplit.split_documents([document])\n",
    "\n",
    "# Step 4 manually assigning the metadata to each split\n",
    "save_dir = \"docs/youtube/\"\n",
    "downloaded_file = [f for f in os.listdir(save_dir) if f.endswith('.m4a')][0]  # Assuming m4a, adjust if using mp3\n",
    "downloaded_file_path = os.path.join(save_dir, downloaded_file)\n",
    "for doc in rdoc2:\n",
    "    doc.metadata = {\"source\": \"youtube\", \"file_path\": downloaded_file_path}\n",
    "\n",
    "\n",
    "# Step 5: Print the first few splits\n",
    "for i, doc in enumerate(rdoc2[30:33]):  # Adjust the number 3 to print more or fewer splits\n",
    "    print(f\"--- Split {i + 1} ---\")\n",
    "    print(doc.page_content)\n",
    "    print()  # Print an empty line for better readability\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aaa78678-8df5-4967-8d44-30efb621df92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata for rdoc1:\n",
      "--- Metadata for Split 1 ---\n",
      "{'producer': 'PDFill: Free PDF Writer and Tools', 'creator': 'PyPDF', 'creationdate': '2011-08-24T10:49:19-04:00', 'moddate': '2011-08-24T10:49:19-04:00', 'source': 'docs/War-of-the-Worlds.pdf', 'total_pages': 128, 'page': 1, 'page_label': '2'}\n",
      "\n",
      "--- Metadata for Split 2 ---\n",
      "{'producer': 'PDFill: Free PDF Writer and Tools', 'creator': 'PyPDF', 'creationdate': '2011-08-24T10:49:19-04:00', 'moddate': '2011-08-24T10:49:19-04:00', 'source': 'docs/War-of-the-Worlds.pdf', 'total_pages': 128, 'page': 1, 'page_label': '2'}\n",
      "\n",
      "--- Metadata for Split 3 ---\n",
      "{'producer': 'PDFill: Free PDF Writer and Tools', 'creator': 'PyPDF', 'creationdate': '2011-08-24T10:49:19-04:00', 'moddate': '2011-08-24T10:49:19-04:00', 'source': 'docs/War-of-the-Worlds.pdf', 'total_pages': 128, 'page': 1, 'page_label': '2'}\n",
      "\n",
      "Metadata for rdoc2:\n",
      "--- Metadata for Split 1 ---\n",
      "{'source': 'youtube', 'file_path': 'docs/youtube/Big Data Architectures.m4a'}\n",
      "\n",
      "--- Metadata for Split 2 ---\n",
      "{'source': 'youtube', 'file_path': 'docs/youtube/Big Data Architectures.m4a'}\n",
      "\n",
      "--- Metadata for Split 3 ---\n",
      "{'source': 'youtube', 'file_path': 'docs/youtube/Big Data Architectures.m4a'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking the metadata\n",
    "\n",
    "# Viewing metadata of the first few splits from rdoc1 (the pdf text)\n",
    "print(\"Metadata for rdoc1:\")\n",
    "for i, doc in enumerate(rdoc1[:3]):  # Adjust the number to view more or fewer splits\n",
    "    print(f\"--- Metadata for Split {i + 1} ---\")\n",
    "    print(doc.metadata)  # Print the metadata\n",
    "    print()  # Print an empty line for better readability\n",
    "\n",
    "# Viewing metadata of the first few splits from rdoc2 (the video transcript)\n",
    "print(\"Metadata for rdoc2:\")\n",
    "for i, doc in enumerate(rdoc2[:3]):  # Adjust the number to view more or fewer splits\n",
    "    print(f\"--- Metadata for Split {i + 1} ---\")\n",
    "    print(doc.metadata)  # Print the metadata\n",
    "    print()  # Print an empty line for better readability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32148e2-3c79-4eb8-943f-bb63fb4e804a",
   "metadata": {},
   "source": [
    "Recursive character splitting is a very common technique. But if you use an LLM that severly limits the number of input token (or charges you b y the token), you may want to split based on tokens instead of character sequences. This is how to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9375610b-1526-4453-9fca-46982cca06a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dd44ea57-3fe6-4c81-8c9b-cf52c600d756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define a very small chunk and no overlap, so you can see what a chunk looks like with this method\n",
    "token_split = TokenTextSplitter(chunk_size=1, chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "04075986-8a0f-408b-a8b1-bef12f58be36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T', 'ruly', ' to', ' speak', ',', ' and', ' with', ' no', ' addition', ',', ' We', ' go', ' to', ' gain', ' a', ' little', ' patch', ' of', ' ground', ' That', ' hath', ' in', ' it', ' no', ' profit', ' but', ' the', ' name', '.', ' To', ' pay', ' five', ' d', 'uc', 'ats', ',', ' five', ',', ' I', ' would', ' not', ' farm', ' it', ';', ' Nor', ' will', ' it', ' yield', ' to', ' Norway', ' or', ' the', ' Pole', ' A', ' rank', 'er', ' rate', ',', ' should', ' it', ' be', ' sold', ' in', ' fee', '.']\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "print(token_split.split_text(Hamlet))\n",
    "print(len(token_split.split_text(Hamlet)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485dca5c-472e-4cce-bda5-7a700c424572",
   "metadata": {},
   "source": [
    "## Storing in Vector Store\n",
    "The third step is to store your splits in a vector database. There are dozens of solutions. Very popular solutions for local storage include Mongodb, Chroma, Weaviate and Milvus. All large Cloud vendors (Azure, AWS etc.) offer a Cloud vectordb solution. Here we use Chroma, a locally stored, flexible popular choice. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482ae5f6-5d15-48d6-b9dc-6353fd4e706d",
   "metadata": {},
   "source": [
    "Before storing our data into the vectordb, we need to convert the text strings into vectors (embedding). We use a tokenizer compatible with the BERT model to first tokenize the text, then embed (convert to vectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b832c59c-75a3-4ab6-a0a4-36a7258d91d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Ollama embeddings and vector store\n",
    "%pip install chromadb\n",
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "all_splits = rdoc1 + rdoc2\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc023e2-ab3e-4d50-a49a-60bb1ca5032c",
   "metadata": {},
   "source": [
    "What do these vectors look like? Let's play with a few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15399743-b027-4b8b-99e9-508ad327ffe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"i like hotdogs\"\n",
    "text2 = \"i like sandwiches\"\n",
    "text3 = \"this is a large building\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360ae3f0-d86d-44ac-9e87-81bc0f597873",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding1 = embeddings.embed_query(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dd83f4-8a8c-4054-9d0d-90fbcee01fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding1 = embeddings.embed_query(text1)\n",
    "embedding2 = embeddings.embed_query(text2)\n",
    "embedding3 = embeddings.embed_query(text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbc5b00-af1c-45af-9bb7-2528052b012a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at the first values of the first embedding\n",
    "print(\"embedding1 includes\", len(embedding1), \"values\")\n",
    "print(\"First few values:\", embedding1[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21485d03-8e16-404e-982d-8dd292b1416c",
   "metadata": {},
   "source": [
    "How closes are these vectors from one another? There are many ways to compare them, here we use the cosine similarity method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2afb0f-944f-4b99-bfdb-fb6940a33a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "# Step 1 : creating the normalized vectors (so the product is between 0 and 1)\n",
    "\n",
    "norm_a = np.linalg.norm(embedding1)\n",
    "norm_b = np.linalg.norm(embedding2)\n",
    "norm_c = np.linalg.norm(embedding3)\n",
    "normalized_a = embedding1 / norm_a\n",
    "normalized_b = embedding2 / norm_b\n",
    "normalized_c = embedding3 / norm_c\n",
    "\n",
    "#Step 2: comparing text1 and text 2 embeddings, then text1 and text 3 embeddings:\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return dot(a, b) / (norm(a) * norm(b))\n",
    "\n",
    "similarity_1_2 = cosine_similarity(embedding1, embedding2)\n",
    "similarity_1_3 = cosine_similarity(embedding1, embedding3)\n",
    "\n",
    "print(\"Similarity (with cos similarity) between sentence 1 and 2:\", similarity_1_2)\n",
    "print(\"Similarity (with cos similarity) between sentence 1 and 3:\", similarity_1_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e43aaa-86a7-4e60-adeb-d276fd3da258",
   "metadata": {},
   "source": [
    "Now that we have embeddings, let's store them into a Chroma database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39875f5a-4dfb-47fd-88f2-5197d15392b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade langchain chromadb\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Set the environment variable to disable tokenizers parallelism and avoid warnings\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Let's define a directory where we'll store the database beyond this notebook execution (and let's make sure it is emtpy, as I run this notebook often :))\n",
    "persist_directory = 'docs/chroma/'\n",
    "!rm -rf ./docs/chroma  # remove old database files if any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6a4179-8f4c-4971-b201-85b08f3edd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_documents(\n",
    "    documents=all_splits,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=persist_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ef1288-a535-4058-89b8-d922781634d4",
   "metadata": {},
   "source": [
    "Now let's see if we can perform some similarity search with this database. keep in mind that we are just comparing vectors here, there is no LLM yet to smartly correlate deeper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5071c834-10b8-4acc-baca-fc1ae4fe022c",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Did the spaceship come from the planet Mars?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8c9919-302b-472c-ba83-0a69d06a482f",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = vectordb.similarity_search(question,k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2299dabf-d085-4f22-be36-38eb8caa3996",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b18a45-8cdd-4037-a229-07312f0ed401",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d124340-1dd2-4e56-908f-202fc48979f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's save the vectordb so we can use it outside of this notebook - note, this is FYI as it is automatically done with Chroma, but not with all other vectordbs!\n",
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ef8da9-7d92-4281-a684-85b6dda2fa30",
   "metadata": {},
   "source": [
    "## Retrieving with the LLM in action\n",
    "The full process consists of asking a question, retrieving the relevant information, then passing the information and the question to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbccfd8-b360-492a-9430-c3b76f048c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We still need these bricks, so do not run this part of the notebook in isolation\n",
    "persist_directory = 'docs/chroma/'\n",
    "embedding = embeddings\n",
    "vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3867d02c-7875-41cd-a119-20e4f696b977",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62fe520-20ec-4f1b-b2e9-cf6bd4c85032",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Did the spaceship come from the planet Mars?\"\n",
    "docs = vectordb.similarity_search(question,k=3)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252cad83-2c21-4f36-96fa-f235bf2b3e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ollama\n",
    "%ollama serve & ollama pull llama3 & ollama pull nomic-embed-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e2be6d-4d44-4d5f-bc92-d1e54c47ffb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Llama3 as the LLM, and Ollama as the wrapper to interact with Llama3. Then using a test question to calidate the install.\n",
    "from langchain_community.llms import Ollama\n",
    "llm = Ollama(model = \"llama3\")\n",
    "llm.invoke(\"Are there aliens on Mars?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529ca15f-1519-491d-8b1b-0c5f5be3e86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ollama langchain beautifulsoup4 chromadb gradio -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e4f3da-8007-4b84-b7fd-15647e5981d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is \"almost\" the final code. You will see the final code in the last lesson of the course\n",
    "import gradio as gr\n",
    "import ollama\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "# Create Ollama embeddings and vector store\n",
    "#embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "#vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)\n",
    "\n",
    "# Define the function to call the Ollama Llama3 model\n",
    "def ollama_llm(question, context):\n",
    "    formatted_prompt = f\"Question: {question}\\n\\nContext: {context}\"\n",
    "    response = ollama.chat(model='llama3', messages=[{'role': 'user', 'content': formatted_prompt}])\n",
    "    return response['message']['content']\n",
    "\n",
    "# Define the RAG setup\n",
    "retriever = vectordb.as_retriever()\n",
    "\n",
    "def rag_chain(question):\n",
    "    retrieved_docs = retriever.invoke(question)\n",
    "    formatted_context = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "    return ollama_llm(question, formatted_context)\n",
    "\n",
    "# Define the Gradio interface\n",
    "def get_important_facts(question):\n",
    "    return rag_chain(question)\n",
    "\n",
    "# Create a Gradio app interface\n",
    "iface = gr.Interface(\n",
    "  fn=get_important_facts,\n",
    "  inputs=gr.Textbox(lines=2, placeholder=\"Enter your question here...\"),\n",
    "  outputs=\"text\",\n",
    "  title=\"RAG with Llama3\",\n",
    "  description=\"Ask questions about the provided context\",\n",
    ")\n",
    "\n",
    "# Launch the Gradio app\n",
    "iface.launch()\n",
    "# example q: did the aliens eventually go on to land on Venus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369fc91c-e24b-4638-86d2-37627a33af44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0670c45b-f553-48cf-b577-f443459f33a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
